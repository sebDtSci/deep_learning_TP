{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'activation ReLU\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Dérivée de ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couche entièrement connectée (Dense Layer)\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.random.randn(1, output_size) * 0.1\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(input, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.biases -= learning_rate * np.sum(output_gradient, axis=0, keepdims=True)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de perte (Cross-Entropy)\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    return -np.mean(np.sum(labels * np.log(predictions), axis=1))\n",
    "\n",
    "# Dérivée de la fonction de perte\n",
    "def cross_entropy_loss_derivative(predictions, labels):\n",
    "    return predictions - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement et prétraitement des données MNIST\n",
    "def load_mnist_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train / 255.0  # Normalisation entre 0 et 1\n",
    "    x_test = x_test / 255.0\n",
    "\n",
    "    # Convertir les labels en encodage one-hot\n",
    "    y_train_one_hot = np.eye(10)[y_train]\n",
    "    y_test_one_hot = np.eye(10)[y_test]\n",
    "    \n",
    "    return (x_train, y_train_one_hot), (x_test, y_test_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réseau de neurones avec votre couche convolutionnelle\n",
    "class SimpleCNN:\n",
    "    def __init__(self):\n",
    "        self.conv = Convolutional(input_shape=(1, 28, 28), kernel_size=3, depth=8)  # Convolution\n",
    "        self.dense = Dense(1352, 10)  # Couche dense pour 10 classes (28-3+1)² * 8 = 1352\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape((-1, 1, 28, 28))  # Redimensionner pour correspondre à la forme d'entrée de la convolution\n",
    "        x = self.conv.forward(x[0])  # Propagation avant de la couche convolutionnelle\n",
    "        x = relu(x)  # Activation ReLU\n",
    "        x = x.flatten()  # Mise à plat\n",
    "        x = self.dense.forward(x)  # Propagation avant de la couche dense\n",
    "        return softmax(x)  # Fonction softmax pour la sortie (probabilités)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        gradient = self.dense.backward(output_gradient, learning_rate)\n",
    "        gradient = gradient.reshape((self.conv.output_shape))  # Reshape to match convolution output\n",
    "        gradient = relu_derivative(self.conv.output) * gradient  # Appliquer dérivée de ReLU\n",
    "        self.conv.backward(gradient, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle sur MNIST\n",
    "def train(model, x_train, y_train, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            # Propagation avant\n",
    "            output = model.forward(x_train[i])\n",
    "\n",
    "            # Calcul de la perte\n",
    "            loss += cross_entropy_loss(output, y_train[i])\n",
    "\n",
    "            # Propagation arrière\n",
    "            output_gradient = cross_entropy_loss_derivative(output, y_train[i])\n",
    "            model.backward(output_gradient, learning_rate)\n",
    "\n",
    "        # Afficher la perte moyenne pour chaque epoch\n",
    "        loss /= len(x_train)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test du modèle\n",
    "def test(model, x_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(x_test)):\n",
    "        output = model.forward(x_test[i])\n",
    "        if np.argmax(output) == np.argmax(y_test[i]):\n",
    "            correct_predictions += 1\n",
    "    accuracy = correct_predictions / len(x_test)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données MNIST\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist_data()\n",
    "# Créer le modèle CNN\n",
    "model = SimpleCNN()\n",
    "# Entraîner le modèle\n",
    "train(model, x_train, y_train, epochs=3, learning_rate=0.01)\n",
    "# Tester le modèle\n",
    "test(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
